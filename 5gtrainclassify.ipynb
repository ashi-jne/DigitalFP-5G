{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "P0EYYALNYh1B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc , confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, ReLU, Add, Dense, Conv2D, Flatten, AveragePooling2D\n",
        "\n",
        "\n",
        "\n",
        "import h5py\n",
        "from numpy import sum,sqrt\n",
        "from numpy.random import standard_normal, uniform\n",
        "\n",
        "from scipy import signal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "gz4G6onrY7px"
      },
      "outputs": [],
      "source": [
        "#datasetpreparation\n",
        "\n",
        "def awgn(data, snr_range):\n",
        "\n",
        "    pkt_num = data.shape[0]\n",
        "    SNRdB = uniform(snr_range[0],snr_range[-1],pkt_num)\n",
        "    for pktIdx in range(pkt_num):\n",
        "        s = data[pktIdx]\n",
        "        # SNRdB = uniform(snr_range[0],snr_range[-1])\n",
        "        SNR_linear = 10**(SNRdB[pktIdx]/10)\n",
        "        P= sum(abs(s)**2)/len(s)\n",
        "        N0=P/SNR_linear\n",
        "        n = sqrt(N0/2)*(standard_normal(len(s))+1j*standard_normal(len(s)))\n",
        "        data[pktIdx] = s + n\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "751qHYtzZfdt"
      },
      "outputs": [],
      "source": [
        "class LoadDataset():\n",
        "    def __init__(self,):\n",
        "        self.dataset_name = 'data'\n",
        "        self.labelset_name = 'label'\n",
        "\n",
        "    def _convert_to_complex(self, data):\n",
        "        '''Convert the loaded data to complex IQ samples.'''\n",
        "        num_row = data.shape[0]\n",
        "        num_col = data.shape[1]\n",
        "        data_complex = np.zeros([num_row,round(num_col/2)],dtype=complex)\n",
        "        data_complex = data[:, ::2] + 1j * data[:, 1::2]\n",
        "\n",
        "\n",
        "        # data_complex = data[:,:round(num_col/2)] + 1j*data[:,round(num_col/2):]\n",
        "        return data_complex\n",
        "\n",
        "    def load_iq_samples(self, file_path, dev_range, pkt_range):\n",
        "        '''\n",
        "        Load IQ samples from a dataset.\n",
        "\n",
        "        INPUT:\n",
        "            FILE_PATH is the dataset path.\n",
        "\n",
        "            DEV_RANGE specifies the loaded device range.\n",
        "\n",
        "            PKT_RANGE specifies the loaded packets range.\n",
        "\n",
        "        RETURN:\n",
        "            DATA is the laoded complex IQ samples.\n",
        "\n",
        "            LABLE is the true label of each received packet.\n",
        "        '''\n",
        "\n",
        "        f = h5py.File(file_path,'r')\n",
        "        label = f[self.labelset_name][:]\n",
        "        label = label.astype(int)\n",
        "        label = np.transpose(label)\n",
        "        label = label - 1\n",
        "\n",
        "        label_start = int(label[0]) + 1\n",
        "        label_end = int(label[-1]) + 1\n",
        "        num_dev = label_end - label_start + 1\n",
        "        num_pkt = len(label)\n",
        "        num_pkt_per_dev = int(num_pkt/num_dev)\n",
        "\n",
        "        print('Dataset information: Dev ' + str(label_start) + ' to Dev ' +\n",
        "              str(label_end) + ', ' + str(num_pkt_per_dev) + ' packets per device.')\n",
        "\n",
        "        sample_index_list = []\n",
        "\n",
        "        for dev_idx in dev_range:\n",
        "            sample_index_dev = np.where(label==dev_idx)[0][pkt_range].tolist()\n",
        "            sample_index_list.extend(sample_index_dev)\n",
        "\n",
        "        data = f[self.dataset_name][sample_index_list]\n",
        "        data = self._convert_to_complex(data)\n",
        "\n",
        "        label = label[sample_index_list]\n",
        "\n",
        "        f.close()\n",
        "        return data,label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "UJXMTubgZmg4"
      },
      "outputs": [],
      "source": [
        "class ChannelIndSpectrogram():\n",
        "    def __init__(self,):\n",
        "        pass\n",
        "\n",
        "    def _normalization(self,data):\n",
        "        ''' Normalize the signal.'''\n",
        "        s_norm = np.zeros(data.shape, dtype=complex)\n",
        "\n",
        "        for i in range(data.shape[0]):\n",
        "\n",
        "            sig_amplitude = np.abs(data[i])\n",
        "            rms = np.sqrt(np.mean(sig_amplitude**2))\n",
        "            s_norm[i] = data[i]/rms\n",
        "\n",
        "        return s_norm\n",
        "\n",
        "    def _spec_crop(self, x):\n",
        "        '''Crop the generated channel independent spectrogram.'''\n",
        "        num_row = x.shape[0]\n",
        "        x_cropped = x[round(num_row*0.3):round(num_row*0.7)]\n",
        "\n",
        "        return x_cropped\n",
        "\n",
        "\n",
        "    def _gen_single_channel_ind_spectrogram(self, sig, win_len=256, overlap=128):\n",
        "        '''\n",
        "        _gen_single_channel_ind_spectrogram converts the IQ samples to a channel\n",
        "        independent spectrogram according to set window and overlap length.\n",
        "\n",
        "        INPUT:\n",
        "            SIG is the complex IQ samples.\n",
        "\n",
        "            WIN_LEN is the window length used in STFT.\n",
        "\n",
        "            OVERLAP is the overlap length used in STFT.\n",
        "\n",
        "        RETURN:\n",
        "\n",
        "            CHAN_IND_SPEC_AMP is the genereated channel independent spectrogram.\n",
        "        '''\n",
        "        # Short-time Fourier transform (STFT).\n",
        "        f, t, spec = signal.stft(sig,\n",
        "                                window='boxcar',\n",
        "                                nperseg= win_len,\n",
        "                                noverlap= overlap,\n",
        "                                nfft= win_len,\n",
        "                                return_onesided=False,\n",
        "                                padded = False,\n",
        "                                boundary = None)\n",
        "\n",
        "        # FFT shift to adjust the central frequency.\n",
        "        spec = np.fft.fftshift(spec, axes=0)\n",
        "\n",
        "        # Generate channel independent spectrogram.\n",
        "        chan_ind_spec = spec[:,1:]/spec[:,:-1]\n",
        "\n",
        "        # Take the logarithm of the magnitude.\n",
        "        chan_ind_spec_amp = np.log10(np.abs(chan_ind_spec)**2)\n",
        "\n",
        "        return chan_ind_spec_amp\n",
        "\n",
        "\n",
        "\n",
        "    def channel_ind_spectrogram(self, data):\n",
        "        '''\n",
        "        channel_ind_spectrogram converts IQ samples to channel independent\n",
        "        spectrograms.\n",
        "\n",
        "        INPUT:\n",
        "            DATA is the IQ samples.\n",
        "\n",
        "        RETURN:\n",
        "            DATA_CHANNEL_IND_SPEC is channel independent spectrograms.\n",
        "        '''\n",
        "\n",
        "        # Normalize the IQ samples.\n",
        "        data = self._normalization(data)\n",
        "\n",
        "        # Calculate the size of channel independent spectrograms.\n",
        "        num_sample = data.shape[0]\n",
        "        num_row = int(256*0.4)\n",
        "        num_column = int(np.floor((data.shape[1]-256)/128 + 1) - 1)\n",
        "        data_channel_ind_spec = np.zeros([num_sample, num_row, num_column, 1])\n",
        "\n",
        "        # Convert each packet (IQ samples) to a channel independent spectrogram.\n",
        "        for i in range(num_sample):\n",
        "\n",
        "            chan_ind_spec_amp = self._gen_single_channel_ind_spectrogram(data[i])\n",
        "            chan_ind_spec_amp = self._spec_crop(chan_ind_spec_amp)\n",
        "            data_channel_ind_spec[i,:,:,0] = chan_ind_spec_amp\n",
        "\n",
        "        return data_channel_ind_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "TiryXvHNZuP6"
      },
      "outputs": [],
      "source": [
        "#deeplearningmodels\n",
        "\n",
        "def resblock(x, kernelsize, filters, first_layer = False):\n",
        "\n",
        "    if first_layer:\n",
        "        fx = Conv2D(filters, kernelsize, padding='same')(x)\n",
        "        fx = ReLU()(fx)\n",
        "        fx = Conv2D(filters, kernelsize, padding='same')(fx)\n",
        "\n",
        "        x = Conv2D(filters, 1, padding='same')(x)\n",
        "\n",
        "        out = Add()([x,fx])\n",
        "        out = ReLU()(out)\n",
        "    else:\n",
        "        fx = Conv2D(filters, kernelsize, padding='same')(x)\n",
        "        fx = ReLU()(fx)\n",
        "        fx = Conv2D(filters, kernelsize, padding='same')(fx)\n",
        "\n",
        "\n",
        "        out = Add()([x,fx])\n",
        "        out = ReLU()(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "def identity_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "jCe94EZ6aHhl"
      },
      "outputs": [],
      "source": [
        "class TripletNet():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def create_triplet_net(self, embedding_net, alpha):\n",
        "\n",
        "#        embedding_net = encoder()\n",
        "        self.alpha = alpha\n",
        "\n",
        "        input_1 = Input([self.datashape[1],self.datashape[2],self.datashape[3]])\n",
        "        input_2 = Input([self.datashape[1],self.datashape[2],self.datashape[3]])\n",
        "        input_3 = Input([self.datashape[1],self.datashape[2],self.datashape[3]])\n",
        "\n",
        "        A = embedding_net(input_1)\n",
        "        P = embedding_net(input_2)\n",
        "        N = embedding_net(input_3)\n",
        "\n",
        "        loss = Lambda(self.triplet_loss, output_shape=lambda s: (s[0][0],))([A, P, N])\n",
        "        model = Model(inputs=[input_1, input_2, input_3], outputs=loss)\n",
        "        return model\n",
        "\n",
        "    def triplet_loss(self,x):\n",
        "    # Triplet Loss function.\n",
        "        anchor,positive,negative = x\n",
        "#        K.l2_normalize\n",
        "    # distance between the anchor and the positive\n",
        "        pos_dist = tf.reduce_sum(tf.square(anchor-positive),axis=1)\n",
        "    # distance between the anchor and the negative\n",
        "        neg_dist = tf.reduce_sum(tf.square(anchor-negative),axis=1)\n",
        "\n",
        "        basic_loss = pos_dist-neg_dist + self.alpha\n",
        "        loss = tf.maximum(basic_loss,0.0)\n",
        "        return loss\n",
        "\n",
        "    def feature_extractor(self, datashape):\n",
        "\n",
        "        self.datashape = datashape\n",
        "\n",
        "        inputs = Input(shape=([self.datashape[1],self.datashape[2],self.datashape[3]]))\n",
        "\n",
        "        x = Conv2D(32, 7, strides = 2, activation='relu', padding='same')(inputs)\n",
        "\n",
        "        x = resblock(x, 3, 32)\n",
        "        x = resblock(x, 3, 32)\n",
        "\n",
        "        x = resblock(x, 3, 64, first_layer = True)\n",
        "        x = resblock(x, 3, 64)\n",
        "\n",
        "        x = AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        x = Dense(512)(x)\n",
        "\n",
        "        #outputs = Lambda(lambda  x: tf.nn.l2_normalize(x,axis=1), output_shape=(512,))(x)\n",
        "        outputs = Lambda(tf.nn.l2_normalize, arguments={'axis': 1}, output_shape=(512,))(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "\n",
        "    def get_triplet(self):\n",
        "        \"\"\"Choose a triplet (anchor, positive, negative) of images\n",
        "        such that anchor and positive have the same label and\n",
        "        anchor and negative have different labels.\"\"\"\n",
        "\n",
        "\n",
        "        n = a = self.dev_range[np.random.randint(len(self.dev_range))]\n",
        "\n",
        "        while n == a:\n",
        "            # keep searching randomly!\n",
        "            n = self.dev_range[np.random.randint(len(self.dev_range))]\n",
        "        a, p = self.call_sample(a), self.call_sample(a)\n",
        "        n = self.call_sample(n)\n",
        "\n",
        "        return a, p, n\n",
        "\n",
        "\n",
        "    def call_sample(self,label_name):\n",
        "        \"\"\"Choose an image from our training or test data with the\n",
        "        given label.\"\"\"\n",
        "        num_sample = len(self.label)\n",
        "        idx = np.random.randint(num_sample)\n",
        "        while self.label[idx] != label_name:\n",
        "            # keep searching randomly!\n",
        "            idx = np.random.randint(num_sample)\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "    def create_generator(self, batchsize, dev_range, data, label):\n",
        "        \"\"\"Generate a triplets generator for training.\"\"\"\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "        self.dev_range = dev_range\n",
        "\n",
        "        while True:\n",
        "            list_a = []\n",
        "            list_p = []\n",
        "            list_n = []\n",
        "\n",
        "            for i in range(batchsize):\n",
        "                a, p, n = self.get_triplet()\n",
        "                list_a.append(a)\n",
        "                list_p.append(p)\n",
        "                list_n.append(n)\n",
        "\n",
        "            A = np.array(list_a, dtype='float32')\n",
        "            P = np.array(list_p, dtype='float32')\n",
        "            N = np.array(list_n, dtype='float32')\n",
        "\n",
        "           # a \"dummy\" label which will come in to our identity loss\n",
        "           # function below as y_true. We'll ignore it.\n",
        "            label = np.ones(batchsize)\n",
        "            yield (A, P, N), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "WDcekLKwaWR4"
      },
      "outputs": [],
      "source": [
        "def train_feature_extractor(\n",
        "        file_path = './dataset/Train/trainset_5G.h5',\n",
        "        dev_range = np.arange(0,3, dtype = int),\n",
        "        pkt_range = np.arange(0,70, dtype = int),\n",
        "        snr_range = np.arange(20,80)\n",
        "                            ):\n",
        "    '''\n",
        "    train_feature_extractor trains an RFF extractor using triplet loss.\n",
        "\n",
        "    INPUT:\n",
        "        FILE_PATH is the path of training dataset.\n",
        "\n",
        "        DEV_RANGE is the label range of LoRa devices to train the RFF extractor.\n",
        "\n",
        "        PKT_RANGE is the range of packets from each LoRa device to train the RFF extractor.\n",
        "\n",
        "        SNR_RANGE is the SNR range used in data augmentation.\n",
        "\n",
        "    RETURN:\n",
        "        FEATURE_EXTRACTOR is the RFF extractor which can extract features from\n",
        "        channel-independent spectrograms.\n",
        "    '''\n",
        "\n",
        "    LoadDatasetObj = LoadDataset()\n",
        "\n",
        "    # Load preamble IQ samples and labels.\n",
        "    data, label = LoadDatasetObj.load_iq_samples(file_path,\n",
        "                                                 dev_range,\n",
        "                                                 pkt_range)\n",
        "\n",
        "    # Add additive Gaussian noise to the IQ samples.\n",
        "    #data = awgn(data, snr_range)\n",
        "\n",
        "    ChannelIndSpectrogramObj = ChannelIndSpectrogram()\n",
        "\n",
        "    # Convert time-domain IQ samples to channel-independent spectrograms.\n",
        "    data = ChannelIndSpectrogramObj.channel_ind_spectrogram(data)\n",
        "\n",
        "    # Specify hyper-parameters during training.\n",
        "    margin = 0.1\n",
        "    #batch_size = 32\n",
        "    batch_size = 10\n",
        "    patience = 20\n",
        "\n",
        "    TripletNetObj = TripletNet()\n",
        "\n",
        "    # Create an RFF extractor.\n",
        "    feature_extractor = TripletNetObj.feature_extractor(data.shape)\n",
        "\n",
        "    # Create the Triplet net using the RFF extractor.\n",
        "    triplet_net = TripletNetObj.create_triplet_net(feature_extractor, margin)\n",
        "\n",
        "    # Create callbacks during training. The training stops when validation loss\n",
        "    # does not decrease for 30 epochs.\n",
        "    early_stop = EarlyStopping('val_loss',\n",
        "                               min_delta = 0,\n",
        "                               patience =\n",
        "                               patience)\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau('val_loss',\n",
        "                                  min_delta = 0,\n",
        "                                  factor = 0.2,\n",
        "                                  patience = 10,\n",
        "                                  verbose=1)\n",
        "    callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "    # Split the dasetset into validation and training sets.\n",
        "    data_train, data_valid, label_train, label_valid = train_test_split(data,\n",
        "                                                                        label,\n",
        "                                                                        test_size=0.1,\n",
        "                                                                        shuffle= True)\n",
        "    del data, label\n",
        "\n",
        "    # Create the trainining generator.\n",
        "    train_generator = TripletNetObj.create_generator(batch_size,\n",
        "                                                     dev_range,\n",
        "                                                     data_train,\n",
        "                                                     label_train)\n",
        "    # Create the validation generator.\n",
        "    valid_generator = TripletNetObj.create_generator(batch_size,\n",
        "                                                     dev_range,\n",
        "                                                     data_valid,\n",
        "                                                     label_valid)\n",
        "\n",
        "\n",
        "    # Use the RMSprop optimizer for training.\n",
        "    opt = RMSprop(learning_rate=1e-3)\n",
        "    triplet_net.compile(loss = identity_loss, optimizer = opt)\n",
        "\n",
        "    # Start training.\n",
        "    history = triplet_net.fit(train_generator,\n",
        "                              steps_per_epoch = data_train.shape[0]//batch_size,\n",
        "                              epochs = 1000,\n",
        "                              validation_data = valid_generator,\n",
        "                              validation_steps = data_valid.shape[0]//batch_size,\n",
        "                              verbose=1,\n",
        "                              callbacks = callbacks)\n",
        "\n",
        "    return feature_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "YFHSIwkJah3z"
      },
      "outputs": [],
      "source": [
        "def test_classification(\n",
        "        file_path_enrol,\n",
        "        file_path_clf,\n",
        "        feature_extractor_name,\n",
        "        dev_range_enrol = np.arange(0,3, dtype = int),\n",
        "        pkt_range_enrol = np.arange(0,70, dtype = int),\n",
        "        dev_range_clf = np.arange(0,3, dtype = int),\n",
        "        pkt_range_clf = np.arange(0,30, dtype = int)\n",
        "                        ):\n",
        "    '''\n",
        "    test_classification performs a classification task and returns the\n",
        "    classification accuracy.\n",
        "\n",
        "    INPUT:\n",
        "        FILE_PATH_ENROL is the path of enrollment dataset.\n",
        "\n",
        "        FILE_PATH_CLF is the path of classification dataset.\n",
        "\n",
        "        FEATURE_EXTRACTOR_NAME is the name of RFF extractor used during\n",
        "        enrollment and classification.\n",
        "\n",
        "        DEV_RANGE_ENROL is the label range of LoRa devices during enrollment.\n",
        "\n",
        "        PKT_RANGE_ENROL is the range of packets from each LoRa device during enrollment.\n",
        "\n",
        "        DEV_RANGE_CLF is the label range of LoRa devices during classification.\n",
        "\n",
        "        PKT_RANGE_CLF is the range of packets from each LoRa device during classification.\n",
        "\n",
        "    RETURN:\n",
        "        PRED_LABEL is the list of predicted labels.\n",
        "\n",
        "        TRUE_LABEL is the list true labels.\n",
        "\n",
        "        ACC is the overall classification accuracy.\n",
        "    '''\n",
        "\n",
        "    # Load the saved RFF extractor.\n",
        "    #feature_extractor = load_model(feature_extractor_name, compile=False)\n",
        "    custom_objects = {'l2_normalize': tf.nn.l2_normalize}\n",
        "    feature_extractor = load_model(feature_extractor_name, compile=False, custom_objects=custom_objects)\n",
        "\n",
        "    LoadDatasetObj = LoadDataset()\n",
        "\n",
        "    # Load the enrollment dataset. (IQ samples and labels)\n",
        "    data_enrol, label_enrol = LoadDatasetObj.load_iq_samples(file_path_enrol,\n",
        "                                                             dev_range_enrol,\n",
        "                                                             pkt_range_enrol)\n",
        "\n",
        "    ChannelIndSpectrogramObj = ChannelIndSpectrogram()\n",
        "\n",
        "    # Convert IQ samples to channel independent spectrograms. (enrollment data)\n",
        "    data_enrol = ChannelIndSpectrogramObj.channel_ind_spectrogram(data_enrol)\n",
        "\n",
        "    # # Visualize channel independent spectrogram\n",
        "    # plt.figure()\n",
        "    # sns.heatmap(data_enrol[0,:,:,0],xticklabels=[], yticklabels=[], cmap='Blues', cbar=False)\n",
        "    # plt.gca().invert_yaxis()\n",
        "    # plt.savefig('channel_ind_spectrogram.pdf')\n",
        "\n",
        "    # Extract RFFs from channel independent spectrograms.\n",
        "    feature_enrol = feature_extractor.predict(data_enrol)\n",
        "    del data_enrol\n",
        "\n",
        "    # Create a K-NN classifier using the RFFs extracted from the enrollment dataset.\n",
        "    knnclf=KNeighborsClassifier(n_neighbors=15,metric='euclidean')\n",
        "    knnclf.fit(feature_enrol, np.ravel(label_enrol))\n",
        "\n",
        "\n",
        "    # Load the classification dataset. (IQ samples and labels)\n",
        "    data_clf, true_label = LoadDatasetObj.load_iq_samples(file_path_clf,\n",
        "                                                         dev_range_clf,\n",
        "                                                         pkt_range_clf)\n",
        "\n",
        "    # Convert IQ samples to channel independent spectrograms. (classification data)\n",
        "    data_clf = ChannelIndSpectrogramObj.channel_ind_spectrogram(data_clf)\n",
        "\n",
        "    # Extract RFFs from channel independent spectrograms.\n",
        "    feature_clf = feature_extractor.predict(data_clf)\n",
        "    del data_clf\n",
        "\n",
        "    # Make prediction using the K-NN classifier.\n",
        "    pred_label = knnclf.predict(feature_clf)\n",
        "\n",
        "    # Calculate classification accuracy.\n",
        "    acc = accuracy_score(true_label, pred_label)\n",
        "    print('Overall accuracy = %.4f' % acc)\n",
        "\n",
        "    return pred_label, true_label, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "6Nr0a8sgatot"
      },
      "outputs": [],
      "source": [
        "# def test_rogue_device_detection(\n",
        "#     feature_extractor_name,\n",
        "#     file_path_enrol = './dataset/Test/dataset_residential.h5',\n",
        "#     dev_range_enrol = np.arange(30,40, dtype = int),\n",
        "#     pkt_range_enrol = np.arange(0,100, dtype = int),\n",
        "#     file_path_legitimate = './dataset/Test/dataset_residential.h5',\n",
        "#     dev_range_legitimate = np.arange(30,40, dtype = int),\n",
        "#     pkt_range_legitimate = np.arange(100,200, dtype = int),\n",
        "#     file_path_rogue = './dataset/Test/dataset_rogue.h5',\n",
        "#     dev_range_rogue = np.arange(40,45, dtype = int),\n",
        "#     pkt_range_rogue = np.arange(0,100, dtype = int),\n",
        "#     ):\n",
        "\n",
        "#     '''\n",
        "#     test_rogue_device_detection performs the rogue device detection task using\n",
        "#     a specific RFF extractor. It returns false positive rate (FPR), true\n",
        "#     positive rate (TPR), area under the curve (AUC) and corresponding threshold\n",
        "#     settings.\n",
        "\n",
        "#     INPUT:\n",
        "\n",
        "#         FEATURE_EXTRACTOR_NAME is the name of RFF extractor used in rogue\n",
        "#         device detection.\n",
        "\n",
        "#         FILE_PATH_ENROL is the path of enrollment dataset.\n",
        "\n",
        "#         DEV_RANGE_ENROL is the device index range used in the enrollment stage.\n",
        "\n",
        "#         PKT_RANGE_ENROL is the packet index range used in the enrollment stage.\n",
        "\n",
        "#         FILE_PATH_LEGITIMATE is the path of dataset contains packets from\n",
        "#         legitimate devices.\n",
        "\n",
        "#         DEV_RANGE_LEGITIMATE is the index range of legitimate devices used in\n",
        "#         the rogue device detection stage.\n",
        "\n",
        "#         PKT_RANGE_LEGITIMATE specifies the packet range from legitimate devices\n",
        "#         used in the rogue device detection stage.\n",
        "\n",
        "#         FILE_PATH_ROGUE is the path of dataset contains packets from rogue\n",
        "#         devices.\n",
        "\n",
        "#         DEV_RANGE_ROGUE is the index range of rogue devices used in the rogue\n",
        "#         device detection stage.\n",
        "\n",
        "#         PKT_RANGE_ROGUE specifies the packet range from rogue devices used in\n",
        "#         the rogue device detection stage.\n",
        "\n",
        "#     RETURN:\n",
        "#         FPR is the detection false positive rate.\n",
        "\n",
        "#         TRP is the detection true positive rate.\n",
        "\n",
        "#         ROC_AUC is the area under the ROC curve.\n",
        "\n",
        "#         EER is the equal error rate.\n",
        "\n",
        "#     '''\n",
        "\n",
        "\n",
        "#     def _compute_eer(fpr,tpr,thresholds):\n",
        "#         '''\n",
        "#         _COMPUTE_EER returns equal error rate (EER) and the threshold to reach\n",
        "#         EER point.\n",
        "#         '''\n",
        "#         fnr = 1-tpr\n",
        "#         abs_diffs = np.abs(fpr - fnr)\n",
        "#         min_index = np.argmin(abs_diffs)\n",
        "#         eer = np.mean((fpr[min_index], fnr[min_index]))\n",
        "\n",
        "#         return eer, thresholds[min_index]\n",
        "\n",
        "#     # Load RFF extractor.\n",
        "#     feature_extractor = load_model(feature_extractor_name, compile=False)\n",
        "\n",
        "#     LoadDatasetObj = LoadDataset()\n",
        "\n",
        "#     # Load enrollment dataset.\n",
        "#     data_enrol, label_enrol = LoadDatasetObj.load_iq_samples(file_path_enrol,\n",
        "#                                                              dev_range_enrol,\n",
        "#                                                              pkt_range_enrol)\n",
        "\n",
        "#     ChannelIndSpectrogramObj = ChannelIndSpectrogram()\n",
        "\n",
        "#     # Convert IQ samples to channel independent spectrograms.\n",
        "#     data_enrol = ChannelIndSpectrogramObj.channel_ind_spectrogram(data_enrol)\n",
        "\n",
        "#     # Extract RFFs from cahnnel independent spectrograms.\n",
        "#     feature_enrol = feature_extractor.predict(data_enrol)\n",
        "#     del data_enrol\n",
        "\n",
        "#     # Build a K-NN classifier.\n",
        "#     knnclf=KNeighborsClassifier(n_neighbors=15,metric='euclidean')\n",
        "#     knnclf.fit(feature_enrol, np.ravel(label_enrol))\n",
        "\n",
        "#     # Load the test dataset of legitimate devices.\n",
        "#     data_legitimate, label_legitimate = LoadDatasetObj.load_iq_samples(file_path_legitimate,\n",
        "#                                                                        dev_range_legitimate,\n",
        "#                                                                        pkt_range_legitimate)\n",
        "#     # Load the test dataset of rogue devices.\n",
        "#     data_rogue, label_rogue = LoadDatasetObj.load_iq_samples(file_path_rogue,\n",
        "#                                                              dev_range_rogue,\n",
        "#                                                              pkt_range_rogue)\n",
        "\n",
        "#     # Combine the above two datasets into one dataset containing both rogue\n",
        "#     # and legitimate devices.\n",
        "#     data_test = np.concatenate([data_legitimate,data_rogue])\n",
        "#     label_test = np.concatenate([label_legitimate,label_rogue])\n",
        "\n",
        "#     # Convert IQ samples to channel independent spectrograms.\n",
        "#     data_test = ChannelIndSpectrogramObj.channel_ind_spectrogram(data_test)\n",
        "\n",
        "#     # Extract RFFs from channel independent spectrograms.\n",
        "#     feature_test = feature_extractor.predict(data_test)\n",
        "#     del data_test\n",
        "\n",
        "#     # Find the nearest 15 neighbors in the RFF database and calculate the\n",
        "#     # distances to them.\n",
        "#     distances, indexes = knnclf.kneighbors(feature_test)\n",
        "\n",
        "#     # Calculate the average distance to the nearest 15 neighbors.\n",
        "#     detection_score = distances.mean(axis =1)\n",
        "\n",
        "#     # Label the packets sent from legitimate devices as 1. The rest are sent by rogue devices\n",
        "#     # and are labeled as 0.\n",
        "#     true_label = np.zeros([len(label_test),1])\n",
        "#     true_label[(label_test <= dev_range_legitimate[-1]) & (label_test >= dev_range_legitimate[0])] = 1\n",
        "\n",
        "#     # Compute receiver operating characteristic (ROC).\n",
        "#     fpr, tpr, thresholds = roc_curve(true_label, detection_score, pos_label = 1)\n",
        "\n",
        "#     # The Euc. distance is used as the detection score. The lower the value,\n",
        "#     # the more similar it is. This is opposite with the probability or confidence\n",
        "#     # value used in scikit-learn roc_curve function. Therefore, we need to subtract\n",
        "#     # them from 1.\n",
        "#     fpr = 1-fpr\n",
        "#     tpr = 1-tpr\n",
        "\n",
        "#     # Compute EER.\n",
        "#     eer, _ = _compute_eer(fpr,tpr,thresholds)\n",
        "\n",
        "#     # Compute AUC.\n",
        "#     roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#     return fpr, tpr, roc_auc, eer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYIX7tvma-wi"
      },
      "source": [
        "MAIN FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "XcNzedINa5qE"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mcan only concatenate tuple (not \"TrackedList\") to tuple\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 512), dtype=float32, sparse=False, name=keras_tensor_367>',)\n  • kwargs={'mask': 'None'}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[113], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m test_dev_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Perform the classification task.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pred_label, true_label, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path_enrol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/Train/trainset_5G.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m####RELABEL\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mfile_path_clf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/Test/testset_5G.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m####RELABEL\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43mfeature_extractor_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Extractor5G_1.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Plot the confusion matrix.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m conf_mat \u001b[38;5;241m=\u001b[39m confusion_matrix(true_label, pred_label)\n",
            "Cell \u001b[1;32mIn[111], line 41\u001b[0m, in \u001b[0;36mtest_classification\u001b[1;34m(file_path_enrol, file_path_clf, feature_extractor_name, dev_range_enrol, pkt_range_enrol, dev_range_clf, pkt_range_clf)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Load the saved RFF extractor.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#feature_extractor = load_model(feature_extractor_name, compile=False)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2_normalize\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39ml2_normalize}\n\u001b[1;32m---> 41\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m LoadDatasetObj \u001b[38;5;241m=\u001b[39m LoadDataset()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load the enrollment dataset. (IQ samples and labels)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\saving\\saving_api.py:183\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    177\u001b[0m         filepath,\n\u001b[0;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:133\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    130\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(model_config)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m saving_options\u001b[38;5;241m.\u001b[39mkeras_option_scope(use_legacy_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 133\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     load_weights_from_hdf5_group(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:85\u001b[0m, in \u001b[0;36mmodel_from_config\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# TODO(nkovela): Swap find and replace args during Keras 3.0 release\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Replace keras refs with keras\u001b[39;00m\n\u001b[0;32m     83\u001b[0m config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\legacy\\saving\\serialization.py:495\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    490\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m _find_replace_nested_dict(\n\u001b[0;32m    491\u001b[0m     cls_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_objects\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m--> 495\u001b[0m     deserialized_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobject_registration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration\u001b[38;5;241m.\u001b[39mCustomObjectScope(custom_objects):\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\models\\model.py:517\u001b[0m, in \u001b[0;36mModel.from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\models\\functional.py:536\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[1;34m(cls, config, custom_objects)\u001b[0m\n\u001b[0;32m    534\u001b[0m node_data \u001b[38;5;241m=\u001b[39m node_data_list[node_index]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# If the node does not have all inbound layers\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# available, stop processing and continue later\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\models\\functional.py:483\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_node\u001b[1;34m(layer, node_data)\u001b[0m\n\u001b[0;32m    480\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m deserialize_node(node_data, created_layers)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Call layer on its inputs, thus creating the node\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# and building the layer if needed.\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optree\\ops.py:594\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    592\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    593\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mcan only concatenate tuple (not \"TrackedList\") to tuple\u001b[0m\n\nArguments received by Lambda.call():\n  • args=('<KerasTensor shape=(None, 512), dtype=float32, sparse=False, name=keras_tensor_367>',)\n  • kwargs={'mask': 'None'}"
          ]
        }
      ],
      "source": [
        "# Specifies what task the program runs for.\n",
        "    # 'Train'/'Classification'/'Rogue Device Detection'\n",
        "run_for = 'Classification'\n",
        "\n",
        "if run_for == 'Train':\n",
        "        \n",
        "        # Train an RFF extractor.\n",
        "        feature_extractor = train_feature_extractor()\n",
        "        # Save the trained model.\n",
        "        feature_extractor.save('Extractor5G_1.h5')\n",
        "\n",
        "\n",
        "elif run_for == 'Classification':\n",
        "        \n",
        "        # Specify the device index range for classification.\n",
        "        test_dev_range = np.arange(0,3, dtype = int)\n",
        "\n",
        "        # Perform the classification task.\n",
        "        pred_label, true_label, acc = test_classification(file_path_enrol =\n",
        "                                                          './dataset/Train/trainset_5G.h5',####RELABEL\n",
        "                                                          file_path_clf =\n",
        "                                                          './dataset/Test/testset_5G.h5', ####RELABEL\n",
        "                                                          feature_extractor_name =\n",
        "                                                          './Extractor5G_1.h5')\n",
        "\n",
        "        # Plot the confusion matrix.\n",
        "        conf_mat = confusion_matrix(true_label, pred_label)\n",
        "        classes = test_dev_range + 1\n",
        "\n",
        "        plt.figure()\n",
        "        sns.heatmap(conf_mat, annot=True,\n",
        "                    fmt = 'd', cmap='Blues',\n",
        "                    cbar = False,\n",
        "                    xticklabels=classes,\n",
        "                    yticklabels=classes)\n",
        "        plt.xlabel('Predicted label', fontsize = 20)\n",
        "        plt.ylabel('True label', fontsize = 20)\n",
        "\n",
        "\n",
        "elif run_for == 'Rogue Device Detection':\n",
        "\n",
        "        # Perform rogue device detection task using three RFF extractors.\n",
        "        #fpr, tpr, roc_auc, eer = test_rogue_device_detection('./models/Extractor_1.h5')\n",
        "\n",
        "        # Plot the ROC curves.\n",
        "        plt.figure(figsize=(4.8, 2.8))\n",
        "        plt.xlim(-0.01, 1.02)\n",
        "        plt.ylim(-0.01, 1.02)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(fpr, tpr, label='Extractor 1, AUC = ' +\n",
        "                 str(round(roc_auc,3)) + ', EER = ' + str(round(eer,3)), C='r')\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.title('ROC curve')\n",
        "        plt.legend(loc=4)\n",
        "        # plt.savefig('roc_curve.pdf',bbox_inches='tight')\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
